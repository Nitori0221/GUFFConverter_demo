{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b09d90",
   "metadata": {},
   "source": [
    "# GGUF 変換 & 量子化ノートブック (llama.cpp)\n",
    "\n",
    "このノートブックは **Hugging Face のモデルを GGUF に変換**し、さらに **量子化 (例: Q5_K_M)** を行います。\n",
    "次の前提で動作します:\n",
    "\n",
    "- `llama.cpp` が `/opt/llama.cpp` にあり、ビルド済みバイナリが `/opt/llama.cpp/build/bin` にあること\n",
    "- Python 依存 (transformers / tokenizers / sentencepiece / safetensors / huggingface_hub / numpy) が導入済み\n",
    "- 作業ディレクトリ `/workspace`、出力 `/workspace/out`、HF キャッシュ `/workspace/hf/.cache`\n",
    "- JupyterLab 上で実行（このノートブックは `/workspace/notebooks` に配置想定）\n",
    "\n",
    "ヒント: 事前に環境変数 `HF_HOME=/workspace/hf/.cache` を設定しておくとキャッシュが再利用でき効率的です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886a0f5",
   "metadata": {},
   "source": [
    "# 0. GPUチェック（最初に実行してください）\n",
    "- このセルは **GPU が正しく見えているか** を素早く確認します。\n",
    "- `nvidia-smi` が利用可能なら **GPU名 / メモリ / Compute Capability** を表で表示します。\n",
    "- 利用不可の場合は `/dev/nvidia*` の存在確認にフォールバックします。\n",
    "- ついでに `nvcc --version` と `` も表示します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ab4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. GPUチェック（最初に実行）\n",
    "import os, subprocess, shutil, pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def sh(cmd: str) -> str:\n",
    "    try:\n",
    "        out = subprocess.check_output([\"bash\",\"-lc\", cmd], text=True, stderr=subprocess.STDOUT)\n",
    "        return out\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return e.output or \"\"\n",
    "\n",
    "\n",
    "# nvidia-smi で表を作る\n",
    "q = \"index,name,driver_version,memory.total,memory.free,compute_cap\"\n",
    "out = sh(f\"nvidia-smi --query-gpu={q} --format=csv,noheader,nounits 2>/dev/null || true\").strip()\n",
    "rows = []\n",
    "if out:\n",
    "    for line in out.splitlines():\n",
    "        parts = [p.strip() for p in line.split(\",\")]\n",
    "        if len(parts) >= 6:\n",
    "            rows.append({\n",
    "                \"GPU\": parts[0],\n",
    "                \"Name\": parts[1],\n",
    "                \"Driver\": parts[2],\n",
    "                \"MemTotal(MiB)\": parts[3],\n",
    "                \"MemFree(MiB)\": parts[4],\n",
    "                \"ComputeCap\": parts[5],\n",
    "            })\n",
    "if rows:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(rows)\n",
    "    display(Markdown(\"**Detected GPU(s)**\"))\n",
    "    display(df)\n",
    "    print()\n",
    "else:\n",
    "    # フォールバック：デバイスファイルの存在確認\n",
    "    print(\"`nvidia-smi` が見つからない/利用できません。フォールバック情報:\")\n",
    "    try:\n",
    "        out2 = sh(\"ls -l /dev/nvidia* 2>/dev/null || true\")\n",
    "        print(out2 or \"NVIDIA device files not present.\")\n",
    "    except Exception as e:\n",
    "        print(\"NVIDIA device files not present.\")\n",
    "\n",
    "# CUDA / nvcc\n",
    "nvcc_line = sh(\"nvcc --version 2>/dev/null | tail -n1 || echo 'nvcc not found'\").strip()\n",
    "print(\"nvcc:\", nvcc_line)\n",
    "\n",
    "# llama.cpp のバイナリ存在チェック（簡易）\n",
    "BIN = \"/opt/llama.cpp/build/bin/llama-cli\"\n",
    "if os.path.exists(BIN):\n",
    "    print(\"llama-cli:\", BIN, \"(found)\")\n",
    "else:\n",
    "    print(\"llama-cli: not found at\", BIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d6206",
   "metadata": {},
   "source": [
    "## 1. バージョン確認 / 依存チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "mods = [\"transformers\",\"tokenizers\",\"sentencepiece\",\"safetensors\",\"huggingface_hub\",\"numpy\"]\n",
    "info = {}\n",
    "for m in mods:\n",
    "    try:\n",
    "        mod = __import__(m)\n",
    "        info[m] = getattr(mod, \"__version__\", \"(no __version__)\")\n",
    "    except Exception as e:\n",
    "        info[m] = f\"IMPORT FAILED: {e}\"\n",
    "print(json.dumps(info, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04479a70",
   "metadata": {},
   "source": [
    "## 2. 変換パラメータの設定\n",
    "\n",
    "- `MODEL_REPO` : Hugging Face のモデルID あるいはローカルパス\n",
    "- `OUT_DIR` : 出力先ディレクトリ\n",
    "- `OUT_BASENAME` : 出力ファイル名のベース\n",
    "- `OUTTYPE` : f16 / f32 など\n",
    "\n",
    "例として **sarashina** を既定にしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_REPO = \"sbintuitions/sarashina2.2-3b-instruct-v0.1\"  # 例\n",
    "OUT_DIR = \"/workspace/out\"\n",
    "OUT_BASENAME = \"sarashina3b\"\n",
    "OUTTYPE = \"f16\"  # f16 / f32 など\n",
    "QUANT = \"Q5_K_M\"  # 推奨量子化（バランス良）\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(\"/workspace/hf\", exist_ok=True)\n",
    "os.environ.setdefault(\"HF_HOME\", \"/workspace/hf/.cache\")\n",
    "print(\"HF_HOME=\", os.environ[\"HF_HOME\"]) \n",
    "print(\"OUT_DIR=\", OUT_DIR)\n",
    "print(\"MODEL_REPO=\", MODEL_REPO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded1297",
   "metadata": {},
   "source": [
    "## 3. GGUF (F16) へ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, time, threading, queue, subprocess\n",
    "from pathlib import Path\n",
    "from ipywidgets import FloatProgress, HBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "conv_script  = \"/opt/llama.cpp/convert_hf_to_gguf.py\"\n",
    "outfile_f16  = f\"{OUT_DIR}/{OUT_BASENAME}-{OUTTYPE}.gguf\"\n",
    "\n",
    "# 進捗UI\n",
    "bar = FloatProgress(min=0, max=1, value=0, bar_style='info', layout={'width':'55%'})\n",
    "label_head = HTML(\"<b>ダウンロード/変換:</b>\")\n",
    "label_tail = HTML(\"準備中…\")\n",
    "ui = HBox([label_head, bar, label_tail])\n",
    "display(ui)\n",
    "\n",
    "# 出力サイズの合計（バイト）。ログから検出できなければ None\n",
    "total_bytes = None\n",
    "last_size   = 0\n",
    "stop_flag   = False\n",
    "lines_q     = queue.Queue()\n",
    "\n",
    "# 変換プロセスを静かに起動（警告を抑止）\n",
    "env = dict(os.environ)\n",
    "env[\"PYTHONWARNINGS\"] = \"ignore::FutureWarning,ignore::UserWarning\"\n",
    "cmd = [\n",
    "    sys.executable, \"-u\",\n",
    "    \"-W\", \"ignore::FutureWarning\", \"-W\", \"ignore::UserWarning\",\n",
    "    conv_script, \"--remote\", MODEL_REPO, \"--outtype\", OUTTYPE, \"--outfile\", outfile_f16\n",
    "]\n",
    "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1, env=env)\n",
    "\n",
    "def _reader(stream):\n",
    "    for line in iter(stream.readline, ''):\n",
    "        lines_q.put(line)\n",
    "    stream.close()\n",
    "\n",
    "t_out = threading.Thread(target=_reader, args=(p.stdout,), daemon=True)\n",
    "t_err = threading.Thread(target=_reader, args=(p.stderr,), daemon=True)\n",
    "t_out.start(); t_err.start()\n",
    "\n",
    "# 単位→バイト換算\n",
    "unit_tbl = {'K':1024, 'M':1024**2, 'G':1024**3, 'T':1024**4}\n",
    "find_total = re.compile(r\"total_size\\s*=\\s*([0-9]+(?:\\.[0-9]+)?)\\s*([KMGT])\", re.I)\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    while True:\n",
    "        # ログをパースして total_size を取得（UIには出さない）\n",
    "        while not lines_q.empty():\n",
    "            line = lines_q.get_nowait()\n",
    "            m = find_total.search(line)\n",
    "            if m and total_bytes is None:\n",
    "                val, unit = float(m.group(1)), m.group(2).upper()\n",
    "                total_bytes = int(val * unit_tbl[unit])\n",
    "                # 総サイズが分かったらプログレスを0%に初期化\n",
    "                bar.value = 0\n",
    "                label_tail.value = f\"0.00 / {val:.2f}{unit}B (0%)\"\n",
    "\n",
    "        # 出力ファイルの伸びで%更新\n",
    "        size_now = Path(outfile_f16).stat().st_size if Path(outfile_f16).exists() else 0\n",
    "        if total_bytes:\n",
    "            done = min(size_now / total_bytes, 1.0)\n",
    "            bar.value = done\n",
    "            done_gb = size_now / (1024**3)\n",
    "            total_str = f\"{total_bytes/(1024**3):.2f}GB\"\n",
    "            label_tail.value = f\"{done_gb:.2f} / {total_str} ({done*100:.1f}%)\"\n",
    "        else:\n",
    "            # まだ総サイズが不明：バーは0%のまま、状態だけ表示\n",
    "            label_tail.value = \"メタ情報取得中…（初回は時間がかかります）\"\n",
    "\n",
    "        if p.poll() is not None:\n",
    "            # プロセス終了\n",
    "            break\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # 終了処理\n",
    "    rc = p.wait(timeout=5)\n",
    "    # 最終更新\n",
    "    if Path(outfile_f16).exists():\n",
    "        size_now = Path(outfile_f16).stat().st_size\n",
    "    if total_bytes:\n",
    "        bar.value = 1.0 if rc == 0 else bar.value\n",
    "        done_gb = size_now / (1024**3)\n",
    "        total_str = f\"{total_bytes/(1024**3):.2f}GB\"\n",
    "        label_tail.value = f\"{done_gb:.2f} / {total_str} ({(size_now/total_bytes)*100:.1f}%)\"\n",
    "    else:\n",
    "        # 総サイズ不明のまま終了した場合はサイズのみ表示\n",
    "        label_tail.value = f\"{size_now/(1024**3):.2f}GB 書き出し済み\"\n",
    "\n",
    "    if rc != 0:\n",
    "        # 失敗時のみ最後の少量ログを出す（レイアウト崩れ対策）\n",
    "        out, err = p.communicate(timeout=2)\n",
    "        print(\"\\n\".join((out + \"\\n\" + err).splitlines()[-30:]))\n",
    "        raise SystemExit(\"convert_hf_to_gguf failed\")\n",
    "\n",
    "    bar.bar_style = 'success'\n",
    "    elapsed = time.time() - t0\n",
    "    label_head.value = \"<b>完了:</b>\"\n",
    "    label_tail.value += f\"  — {elapsed/60:.1f} 分\"\n",
    "finally:\n",
    "    stop_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f15368",
   "metadata": {},
   "source": [
    "## 4. 量子化（例：Q5_K_M）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 量子化（例：Q5_K_M） — RUNは素のテキスト表示、進捗バーは継続\n",
    "import os, sys, time, shutil, subprocess, threading, queue\n",
    "from pathlib import Path\n",
    "from ipywidgets import FloatProgress, HBox, HTML\n",
    "from IPython.display import display\n",
    "\n",
    "BIN_DIR = \"/opt/llama.cpp/build/bin\"\n",
    "llama_quantize = shutil.which(\"llama-quantize\") or f\"{BIN_DIR}/llama-quantize\"\n",
    "assert Path(llama_quantize).exists(), f\"llama-quantize が見つかりません: {llama_quantize}\"\n",
    "assert 'outfile_f16' in globals() and Path(outfile_f16).exists(), \"F16 GGUF が見つかりません。先に変換を完了させてください。\"\n",
    "\n",
    "outfile_quant = f\"{OUT_DIR}/{OUT_BASENAME}-{QUANT}.gguf\"\n",
    "\n",
    "# 期待サイズ（概算）\n",
    "bpw_guess = {\"Q8_0\":8.0,\"Q6_K\":6.3,\"Q5_K_M\":5.7,\"Q5_K\":5.5,\"Q4_K_M\":4.8,\"Q4_1\":4.5,\"Q3_K_M\":3.5,\"Q2_K\":2.6}\n",
    "f16_bpw = 16.0\n",
    "bpw = bpw_guess.get(QUANT.upper(), 5.7)\n",
    "in_size = Path(outfile_f16).stat().st_size\n",
    "expected_out = int(in_size * (bpw / f16_bpw))\n",
    "\n",
    "# ── RUN 行（プレーン表示） ─────────────────────────\n",
    "cmd = [llama_quantize, outfile_f16, outfile_quant, QUANT]\n",
    "run_str = \"RUN: \" + \" \".join(cmd)\n",
    "print(run_str)\n",
    "\n",
    "# ── 進捗バー UI ───────────────────────────────────\n",
    "bar = FloatProgress(min=0, max=1, value=0, bar_style='info', layout={'width':'55%'})\n",
    "label_head = HTML(\"<b>量子化:</b>\")\n",
    "label_tail = HTML(\"準備中…\")\n",
    "ui = HBox([label_head, bar, label_tail])\n",
    "display(ui)\n",
    "\n",
    "# ── 実行（静かめ） ─────────────────────────────────\n",
    "env = dict(os.environ)\n",
    "env[\"PYTHONWARNINGS\"] = \"ignore::FutureWarning,ignore::UserWarning\"\n",
    "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1, env=env)\n",
    "\n",
    "lines_q = queue.Queue()\n",
    "def _reader(stream):\n",
    "    for line in iter(stream.readline, ''):\n",
    "        lines_q.put(line)\n",
    "    stream.close()\n",
    "\n",
    "t_out = threading.Thread(target=_reader, args=(p.stdout,), daemon=True)\n",
    "t_err = threading.Thread(target=_reader, args=(p.stderr,), daemon=True)\n",
    "t_out.start(); t_err.start()\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    while True:\n",
    "        if Path(outfile_quant).exists():\n",
    "            written = Path(outfile_quant).stat().st_size\n",
    "            denom = max(expected_out, 1)\n",
    "            frac = min(written / denom, 0.99)  # 完了前は 99% で止める\n",
    "            bar.value = frac\n",
    "            label_tail.value = f\"{written/(1024**3):.2f} / {denom/(1024**3):.2f} GB ({frac*100:.1f}%)\"\n",
    "        else:\n",
    "            label_tail.value = \"出力ファイル作成待ち…\"\n",
    "\n",
    "        if p.poll() is not None:\n",
    "            break\n",
    "        time.sleep(0.25)\n",
    "\n",
    "    rc = p.wait(timeout=5)\n",
    "\n",
    "    if Path(outfile_quant).exists():\n",
    "        written = Path(outfile_quant).stat().st_size\n",
    "        bar.value = 1.0\n",
    "        label_head.value = \"<b>量子化: 完了</b>\"\n",
    "        label_tail.value = f\"{written/(1024**3):.2f} GB  — {(time.time()-t0)/60:.1f} 分\"\n",
    "    else:\n",
    "        label_tail.value = \"出力が生成されませんでした\"\n",
    "\n",
    "    if rc != 0:\n",
    "        out = []; err = []\n",
    "        while not lines_q.empty():\n",
    "            ln = lines_q.get_nowait()\n",
    "            (err if \"error\" in ln.lower() else out).append(ln.rstrip())\n",
    "        tail = \"\\n\".join((out+err)[-30:])\n",
    "        print(\"\\n--- llama-quantize last logs ---\\n\" + tail)\n",
    "        bar.bar_style = 'danger'\n",
    "        raise SystemExit(\"quantize failed\")\n",
    "\n",
    "    bar.bar_style = 'success'\n",
    "    print(f\"OK -> {outfile_quant}\")\n",
    "\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812019e",
   "metadata": {},
   "source": [
    "## 5. 出力確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {OUT_DIR}/*.gguf || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fb3de",
   "metadata": {},
   "source": [
    "## 6. クイック動作確認（任意）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. クイック動作確認（ログ非表示・会話テキストのみ / 先頭復唱の自動除去）\n",
    "import os, shlex, subprocess, shutil, re\n",
    "from ipywidgets import HTML, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "BIN_DIR = \"/opt/llama.cpp/build/bin\"\n",
    "llama_cli = shutil.which(\"llama-cli\") or f\"{BIN_DIR}/llama-cli\"\n",
    "\n",
    "# ===== 調整ポイント =====\n",
    "PROMPT      = \"日本語で自己紹介してください。\"\n",
    "N_PREDICT   = 512\n",
    "CONTEXT_LEN = 4096\n",
    "BATCH       = 256\n",
    "MODEL_PATH  = outfile_quant   # 量子化した GGUF を使う\n",
    "# =======================\n",
    "\n",
    "cmd = [\n",
    "    llama_cli,\n",
    "    \"-m\", MODEL_PATH,\n",
    "    \"--simple-io\",                # 純テキストのみを stdout に\n",
    "    \"-p\", PROMPT,\n",
    "    \"-n\", str(N_PREDICT),\n",
    "    \"-c\", str(CONTEXT_LEN),\n",
    "    \"-b\", str(BATCH),\n",
    "    \"-ngl\", \"999\",\n",
    "    \"--temp\", \"0.7\", \"--top-p\", \"0.95\", \"--seed\", \"0\",\n",
    "]\n",
    "print(\"RUN:\", \" \".join(shlex.quote(x) for x in cmd))\n",
    "\n",
    "# 先頭の「復唱」っぽいテキストを削るフィルタ\n",
    "_prompt_pat = re.escape(PROMPT)\n",
    "ECHO_RE = re.compile(\n",
    "    r\"^\\s*(?:[「『\\\"]?\\s*\"+_prompt_pat+r\"\\s*[」』\\\"]?\\s*[:：]?\\s*){1,3}\\s*\",\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "\n",
    "def drop_leading_echo(text: str) -> str:\n",
    "    # 先頭に連続する復唱（最大3回）をまとめて除去\n",
    "    return ECHO_RE.sub(\"\", text, count=1)\n",
    "\n",
    "# ストリーミング描画\n",
    "html = HTML(\"<div style='font-family:monospace; white-space:pre-wrap; line-height:1.6'></div>\")\n",
    "display(VBox([html]))\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"LLAMA_LOG_LEVEL\"]  = \"1\"   # errorのみ\n",
    "env[\"LLAMA_LOG_COLORS\"] = \"0\"\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "    cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.DEVNULL,   # 赤ログを完全に消す\n",
    "    text=True,\n",
    "    bufsize=0,\n",
    "    env=env,\n",
    ")\n",
    "\n",
    "buf = []\n",
    "try:\n",
    "    while True:\n",
    "        chunk = proc.stdout.read(256)\n",
    "        if not chunk:\n",
    "            if proc.poll() is not None:\n",
    "                break\n",
    "            continue\n",
    "        buf.append(chunk)\n",
    "        text = \"\".join(buf)\n",
    "        # 実行中は “一度だけ” フィルタを試す（復唱が来た瞬間を消す）\n",
    "        text = drop_leading_echo(text)\n",
    "        html.value = f\"<div style='font-family:monospace; white-space:pre-wrap; line-height:1.6'>{text}</div>\"\n",
    "finally:\n",
    "    proc.wait()\n",
    "    text = drop_leading_echo(\"\".join(buf))\n",
    "    html.value = f\"<div style='font-family:monospace; white-space:pre-wrap; line-height:1.6'>{text}</div>\"\n",
    "\n",
    "print(f\"[exit] {proc.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd762a-447a-47d8-939a-ef8fb45dabca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
